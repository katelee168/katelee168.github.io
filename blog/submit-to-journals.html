<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Katherine Lee" />
  <meta name="dcterms.date" content="2018-04-23" />
  <title>Submit to journals</title>
  <link rel="stylesheet" href="../css/newspaper.css" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-165430430-1"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-165430430-1');
  </script>
</head>
<body>
  <div class="container">
    <nav class="navbar">
      <div class="nav-container">
        <div class="nav-brand">
          <a href="../index.html">Katherine Lee</a>
        </div>
        <button class="nav-toggle" aria-label="Toggle navigation">
          <span class="hamburger"></span>
        </button>
        <ul class="nav-menu">
          <li class="nav-item">
            <a href="../index.html#selected-publications" class="nav-link">Publications</a>
          </li>
          <li class="nav-item">
            <a href="../index.html#writing" class="nav-link">Writing</a>
          </li>
          <li class="nav-item">
            <a href="../index.html#invited-talks" class="nav-link">Talks</a>
          </li>
        </ul>
      </div>
    </nav>



    <main class="content">
            
            
      <article class="main-content">
                <h1 class="article-title">Submit to journals</h1>
                        <div class="article-meta">
          <p class="article-date">April 23, 2018</p>
                    <p class="article-author">Katherine Lee</p>
                  </div>
                <p>Submitting to conferences by default sets the
                research timeline to three months<a href="#fn1"
                class="footnote-ref" id="fnref1"
                role="doc-noteref"><sup>1</sup></a>. This is
                unsustainable and does not encourage diverse forms of
                research. As deep learning becomes more mature and
                public, we must think long-term, maintain investments
                made in young researchers, and incentivize seminal work.
                We, as a field, will alleviate pressure, promote
                influential work, and maintain a high bar of research by
                incorporating rolling deadlines while maintaining peer
                review. Thankfully, we already have open-access
                publications that incorporate both rolling deadlines and
                peer review: journals like Journal of Machine Learning
                Research (JMLR), Journal of Artificial Intelligence
                Research (JAIR), Public Library of Science (PLOS),
                Distill, and more. Let’s submit to them more.</p>
                <h3
                id="rolling-deadlines-provide-relief-and-allow-diverse-questions">Rolling
                deadlines provide relief and allow diverse
                questions</h3>
                <p>The three-month timeline creates resource crunches,
                is stressful, and incentivizes work that can be
                completed in three months. Rolling deadlines help
                alleviate these problems and promote higher quality of
                work by forcing researchers to decide when they are
                done.</p>
                <p>During the weeks leading up to a major conference
                deadline, researchers work into the night and shared
                computational resources become strained. Days before the
                deadline, researchers wonder if the science in their
                project will be finished. When everyone submits to the
                same deadlines, researchers are unable to help peers
                review work. The expectation that researchers submit to
                multiple conferences a year means that stressful periods
                before deadlines repeat continuously. Spreading work
                throughout the year through rolling deadlines allows
                better utilization and allocation of human and
                computational resources. When researchers can get
                feedback before submitting papers, they can iterate and
                arrive at more complex insights. Additionally, I have
                observed weeks of low computational resource utilization
                following major conference deadlines. Spreading
                experiments more throughout the year can make resource
                sharing more equitable and efficient.</p>
                <p>A project that is not yet finished during the week
                leading up to the deadline could either be rushed to
                completion, or submitted at the next conference. The
                former incentivizes researchers to submit work before
                they are confident, leaving writing the paper to the
                last hours before the deadline. Pressure to submit can
                cause researchers to neglect performing critical
                experiments, decreasing the strength of technical
                arguments in the paper. With rolling deadlines,
                researchers can take time to be sure the project is
                complete and submit it for review immediately
                afterwards. Now, it could be that deep learning is so
                competitive that the three-month conference timeline
                extends how long researchers spend on a project.
                However, given resources crunches and the number of
                researchers in the office past midnight, it seems
                unlikely that the same types of questions research asks
                today could be done faster.</p>
                <p>The push to publish encourages researchers to start
                projects with the goal of submitting to a particular
                conference, setting a cap on the amount of time
                available. While some questions can be furthered in
                three months, many projects, not building off of larger
                work, will be incremental. The difference between
                starting a six month project and letting a three month
                project roll to the next conference deadline is the
                types of questions researchers ask. The three-month
                timeline means we tend not to explore questions that
                take both shorter and longer to consider. As a new and
                continually evolving field, we need research that lays
                groundwork, such as: developing metrics, doing
                theoretical research, defining terms (what does it mean
                for a model to be interpretable?), debating opinions (to
                what extent should researchers join policy discussions
                on health-care?), and surveying the state of a subfield.
                We need to incentivize a greater diversity of work and,
                in particular, seminal work.</p>
                <h3 id="peer-review-is-valued">Peer review is
                valued</h3>
                <p>If rolling deadlines are the key, why not submit to
                arXiv? Though arXiv allows researchers to publish
                anytime, it does not review individual articles<a
                href="#fn2" class="footnote-ref" id="fnref2"
                role="doc-noteref"><sup>2</sup></a> <span
                class="citation" data-cites="arXiv">[@arXiv]</span>.
                Currently, researchers concurrently submit to arXiv and
                high-impact conferences like NIPS and ICML, or journals
                like Nature<a href="#fn3" class="footnote-ref"
                id="fnref3" role="doc-noteref"><sup>3</sup></a>.
                Researchers continue to use peer-reviewed publications
                even when presented with other options. This signals
                that peer reviews are still valuable as an indication of
                legitimacy.</p>
                <p>A culture of peer review extends beyond the absolute
                number of papers reviewed. Currently, the number of
                submitted publications is overwhelming and gives the
                impression that deep learning is moving forward fast.
                The community uses tools built on top of arXiv, such as
                arXiv Sanity which sorts arXiv articles by hype (the
                number of shares on Twitter), and social media like
                Twitter and Reddit to stay informed of relevant work.
                Reputation for strong peer reviews encourages the
                community to submit good work and may help decrease the
                number of submissions. Additionally, unlike social media
                mentions, peer review explicitly filters for conflicts
                of interest which helps level the playing field.</p>
                <p>We should specifically use journal style reviews with
                options to accept, revise and resubmit, and reject.
                Recommendations to revise and resubmit will help
                researchers incorporate reviews and iteratively refine
                their ideas. Unlike conference review policies, which do
                not ask reviewers to verify that papers incorporated
                suggestions they made, journal-style reviews encourages
                authors to more seriously consider suggestions. Revise
                and resubmit is sometimes mimicked by revising and
                submitting to the next conference. However, since this
                does not ask the same set of people to review the paper
                again<a href="#fn4" class="footnote-ref" id="fnref4"
                role="doc-noteref"><sup>4</sup></a>, it is more akin to
                re-rolling the peer review dice<a href="#fn5"
                class="footnote-ref" id="fnref5"
                role="doc-noteref"><sup>5</sup></a>. Additionally, the
                distinction between revise and resubmit and reject
                provides researchers with more feedback on how to
                progress their work.</p>
                <h3 id="todays-journals-and-conferences">Today’s
                journals and conferences</h3>
                <p>But if journals incorporate rolling submissions and a
                preferable peer review system, why have deep learning
                researchers turned to conferences instead? In the past,
                researchers frequently published in JMLR, giving it an
                impact factor of 7.48 in 2005 <span class="citation"
                data-cites="researchgate">[@researchgate]</span>. Common
                wisdom now states that a good conference paper is on par
                with a journal article. This shift is reflected in
                JMLR’s decreased impact factor along with NIPS, ICML,
                and arXiv’s increased impact [<span class="citation"
                data-cites="researchgate">@researchgate</span>; <span
                class="citation"
                data-cites="googleh5">@googleh5</span>]<a href="#fn6"
                class="footnote-ref" id="fnref6"
                role="doc-noteref"><sup>6</sup></a><a href="#fn7"
                class="footnote-ref" id="fnref7"
                role="doc-noteref"><sup>7</sup></a>. This may be because
                researchers believe publishing in journals is slower for
                the same impact as conferences, conferences provide a
                necessary social component, and younger researchers may
                not have heard of journals at all.</p>
                <p>Going through the journal review process can be slow.
                JMLR, deep learning’s flagship journal, has not had a
                deadline for reviewers and may have given the impression
                that all journal review processes are slow. But journal
                reviews can be done quickly. JAIR claims to have a
                publishing decision in two to three months <span
                class="citation"
                data-cites="jair_submissions">[@jair_submissions]</span>
                and TACL (Transactions of the Association for
                Computational Linguistics) gives reviewers three weeks
                to write a review and publishes the paper when it passes
                review<a href="#fn8" class="footnote-ref" id="fnref8"
                role="doc-noteref"><sup>8</sup></a>. On the contrary,
                the NIPS review process in 2018 from submission to the
                date of the conference was seven months and four months
                from submission to receiving a decision from area chairs
                <span class="citation" data-cites="nips">[@nips]</span>.
                Before NIPS 2017, I heard the following refrains: “My
                work was published months ago and everyone’s read it
                already, but I still have to poster for it, maybe I
                should talk about what I’m doing now instead,” “I’m
                going to NIPS to see people, but I think I’ve seen all
                the interesting papers already.” If rolling-submission
                journals prioritized quick reviews, they would be better
                equipped than conferences to turn around reviews as
                submissions are spaced throughout the year. As an
                example, TACL has a maximum of one paper per reviewer
                and a three-week window to review papers.</p>
                <p>Publication impact is not a clear institutional
                change like prioritizing quick reviews. The prestige of
                a journal is a measure of how much social capital and
                trust the community gives it. Like how rigorous reviews
                attract better submissions, prestigious journals attract
                good work enabling them to become more prestigious and
                drive up metrics like h-5 and its impact factor.
                Prestige and impact can be seeded. All it takes is for a
                few influential papers to be published in the venue to
                attract more. If we submit more to journals, their
                impact factors and prestige will rise.</p>
                <p>This is not to say researchers should not submit to
                conferences. Use conferences for what they are uniquely
                good at: fostering in person research discussions on
                in-progress research or new and controversial research<a
                href="#fn9" class="footnote-ref" id="fnref9"
                role="doc-noteref"><sup>9</sup></a>. By having
                conversations earlier on in the research process, we can
                iterate and refine research ideas with more feedback and
                move further as a field<a href="#fn10"
                class="footnote-ref" id="fnref10"
                role="doc-noteref"><sup>10</sup></a>. Instead of using
                conference deadlines as a forcing function to start and
                finish work, use conference deadlines as motivation to
                write up and set goals for larger forms of work. Submit
                in-progress work to conferences and get feedback from
                peers. The key is: conferences are not the only
                submission venue.</p>
                <p>Additionally, journals need not worry too much about
                fast turnaround time. While journal reviews can be at
                pace or faster than conference reviews, submitting to
                preprint servers, like arXiv, will always be fastest
                since they specialize in dissemination. On the other
                hand, journals specialize in evaluation, and conferences
                are uniquely good at community building, but our
                conferences currently conflate evaluation, community
                building, and, to an extent, dissemination. We may do
                better by having specialized venues. In my ideal world,
                we would separate the goals of dissemination,
                evaluation, and community building. We would use arXiv
                for dissemination, journals for evaluation, and
                conferences for community building. Researchers would
                send work to arXiv the same day they submit to a rolling
                journal for peer review. Work accepted to the journal
                could be presented in a poster session at the next
                affiliated conference. This frees conferences to focus
                on in-person interactions and prioritize workshops
                discussing narrow realms of in-progress research and
                controversial discussions. In my ideal world, we would
                not submit directly to conferences. I am far from the
                first person to propose divorcing dissemination,
                evaluation, and community building. This proposal is
                heavily based off how the journal TACL interacts with
                ACL conferences. Additionally, events like Unconference
                and Dali successfully prompted deeper discussion into
                the state of the field and controversial topics.</p>
                <h3 id="conclusion">Conclusion</h3>
                <p>We can reap most benefits of the ideal: decreased
                pressure and promoting seminal research by submitting to
                JMLR, JAIR, PLOS, Distill, and other open-access
                journals.</p>
                <p>To researchers who have been in the field for longer,
                perhaps the roles of journals and conferences are clear.
                I have observed that many researchers new to deep
                learning are less familiar with journals (“Oh yeah,
                journals, you mean conference publications?”, “JMLR, is
                that like JSTOR?”) and are under the impression that we
                should submit to conferences, or upon missing the
                deadline, put it on arXiv. Ask your students and mentees
                if they are familiar, and if not, please share with your
                students.</p>
                <p>I acknowledge the irony that I am submitting this
                piece to a conference workshop. However, I am hopeful
                that this is the beginning of the discussion on
                conference timelines and journals, and not the end. I
                look forward to hearing your perspectives on how to
                reduce stress and promote diversity of research and a
                hearty round of debate at ICML.</p>
                <h3 id="acknowledgments">Acknowledgments</h3>
                <p>Thank you to Ishaan Gulrajani, David Dohan, Doug Eck,
                James Bradbury, Samy Bengio, Ian Goodfellow, Catherine
                Olsson, Chris Olah, Ludwig Schubert, Colin Raffel, Kevin
                Murphy, Maithra Raghu, Tom Brown, and many others at
                Google Brain for insightful discussions and feedback.
                I’ve learned a lot through this process, and hope we can
                can continue the conversation.</p>
                <h3 id="references">References</h3>
                <!-- we should pour more effort into making journals good -->
                <aside id="footnotes"
                class="footnotes footnotes-end-of-document"
                role="doc-endnotes">
                <hr />
                <ol>
                <li id="fn1"><p>Conference deadlines are not exactly
                three months apart. In the past year, the three largest
                conferences’ submission dates were: ICLR October 27,
                ICML February 9, NIPS May 18. Interwoven between
                conference deadlines are workshop submissions and
                specialized conferences deadlines <span class="citation"
                data-cites="iclr icml nips">[@iclr; @icml;
                @nips]</span>.<a href="#fnref1" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn2"><p>arXiv asks authors to be endorsed by an
                already endorsed author. An endorsed author can submit
                anytime.<a href="#fnref2" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn3"><p>From Nature’s preprint policy: “The
                original submitted version of the manuscript (the
                version that has not undergone peer review) may be
                posted at any time” <span class="citation"
                data-cites="nature_preprint">[@nature_preprint]</span>.<a
                href="#fnref3" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn4"><p>TACL (Transactions of the Association
                for Computational Linguistics), a journal in an adjacent
                field has options to accept, conditionally accept,
                reject but revise and resubmit, and reject. Conditional
                acceptances at TACL will be reviewed by the same
                reviewers. Reject but revise and resubmit may or may not
                have the same reviewers <span class="citation"
                data-cites="tacl_review">[@tacl_review]</span>.<a
                href="#fnref4" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn5"><p>Conference acceptances are thought to
                have some randomness. The NIPS experiment in 2014 found
                that two independent program committees disagreed on
                25.9% of the papers reviewed by both committees <span
                class="citation"
                data-cites="nips_experiment">[@nips_experiment]</span>.<a
                href="#fnref5" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn6"><p>This ranking is done by h-5. From the
                Google Scholar page: “h5-index is the h-index for
                articles published in the last 5 complete years. It is
                the largest number h such that h articles published in
                2012-2016 have at least h citations each” <span
                class="citation"
                data-cites="googleh5">[@googleh5]</span>.<a
                href="#fnref6" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn7"><p>In 2013, JMLR saw 764 submissions <span
                class="citation"
                data-cites="jmlr_stats">[@jmlr_stats]</span>. In 2014,
                NIPS received 1660 submissions. There are no statistics
                on JMLR’s website for submissions past 2013, but NIPS
                2017 saw 3590 submissions <span class="citation"
                data-cites="nips_2017_stats">[@nips_2017_stats]</span>.
                To be absolutely correct, we should look at deep
                learning submissions at JMLR and NIPS; however, I was
                unable to find statistics for this. Anecdotally,
                researchers in 2018 prefer to submit conferences.<a
                href="#fnref7" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn8"><p>Conditional acceptances have two months
                to complete the revisions and then another round of
                reviews (three weeks to review) for an estimated four
                months for two decisions and a revision.<a
                href="#fnref8" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn9"><p>Twitter, while an amazing tool and
                democratizer in the research community, does not replace
                in-person interactions. Both forms of interaction,
                online and in-person, pose their own problems and have
                their own advantages–the exact nature of which has been
                extensively studied and would be a much longer paper to
                discuss. For now, I will simply state that the two are
                not the same.<a href="#fnref9" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                <li id="fn10"><p>Currently, journals allow for work that
                has been previously published in conference proceedings.
                From JMLR’s author info: “Submissions to JMLR cannot
                have been published previously in any other journal. We
                will consider submissions that have been published at
                workshops or conferences. In these cases, we expect the
                JMLR submission to cite the prior work, go into much
                greater depth and to extend the published results in a
                substantive way” <span class="citation"
                data-cites="jmlr_author">[@jmlr_author]</span>.<a
                href="#fnref10" class="footnote-back"
                role="doc-backlink">↩︎</a></p></li>
                </ol>
                </aside>
      </article>
    </main>

    <footer class="footer">
      <div class="footer-content">
        <p>&copy; April 23, 2018 Katherine Lee. All rights reserved.</p>
        <p>Website generated with <a href="https://pandoc.org">Pandoc</a>, <a href="https://gist.github.com/killercup/5917178">pandoc.css</a>, and help from <a href="https://schubert.io/">Ludwig Schubert</a> and <a href="https://cursor.com">Cursor</a>.</p>
      </div>
    </footer>
  </div>


<script>
// Mobile navigation toggle
document.addEventListener('DOMContentLoaded', function() {
  const navToggle = document.querySelector('.nav-toggle');
  const navMenu = document.querySelector('.nav-menu');
  
  navToggle.addEventListener('click', function() {
    navMenu.classList.toggle('active');
    navToggle.classList.toggle('active');
  });
  
  // Close mobile menu when clicking on a link
  const navLinks = document.querySelectorAll('.nav-link');
  navLinks.forEach(link => {
    link.addEventListener('click', () => {
      navMenu.classList.remove('active');
      navToggle.classList.remove('active');
    });
  });
  
  // Close mobile menu when clicking outside
  document.addEventListener('click', function(event) {
    if (!navToggle.contains(event.target) && !navMenu.contains(event.target)) {
      navMenu.classList.remove('active');
      navToggle.classList.remove('active');
    }
  });
});
</script>
</body>
</html> 